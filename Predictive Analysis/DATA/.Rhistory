# Verify the update by selecting the entire table
employee <- sqldf("SELECT * FROM employee")
print("Updated employee data:")
print(employee)
# Update the name of the employee where empid is 100 to 'Arush'
sqldf("UPDATE employee SET name = 'Arush' WHERE empid = 100")
# Update the name of the employee where empid is 100 to 'Arush'
sqldf("UPDATE employee name = 'Arush' WHERE empid = 100")
# Verify the update by selecting the entire table
employee <- sqldf("SELECT * FROM employee")
print("Updated employee data:")
print(employee)
# Find the sum of salaries and display it as 'total sum'
total_salary <- sqldf("SELECT SUM(sal) AS total_sum FROM employee")
print("Total sum of salaries:")
print(total_salary)
# Find the number of employees working in the HR department
hr_count <- sqldf("SELECT COUNT(*) AS hr_employees FROM employee WHERE dept = 'HR'")
print("Number of employees in HR:")
print(hr_count)
# Update the name of the employee where empid is 100 to 'Arush'
employee[employee$empid == 100, "name"] <- 'Arush'
# Verify the update by selecting the entire table
employee <- sqldf("SELECT * FROM employee")
print("Updated employee data:")
print(employee)
?naiveBayes
require(stringi)
library(caret)
library(e1071)
?naiveBayes
knn_data <- read.csv(file.choose(), stringsAsFactors = FALSE)
?factor
wine_data <- read.csv(url, sep = ";")
url_data <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
wine_data <- read.csv(url, sep = ";")
wine_data <- read.csv(url_data, sep = ";")
View(wine_data)
require(class)
require(caret)
url_data <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
wine_data <- read.csv(url_data, sep = ";")
View(wine_data)
require(class)
require(caret)
url_data <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
wine_data <- read.csv(url_data, sep = ";")
View(wine_data)
require(class)
require(caret)
require()
require(class)
require(e1071)
vine_data <- read.csv(url_data, sep = ";")
str(vine_data)
?ifelse
vine_data$quality <- ifelse(vine_data$quality >= 7, "Acidic", "Basic")
vine_norm <- function(x){
(x-min(x))/(max(x)-min(x))
}
vine_norm_new <- as.data.frame(lapply(vine_data[,-20], vine_norm))
vine_norm_new <- as.data.frame(lapply(vine_data[,-12], vine_norm))
# Check structure of the dataset
str(wine_data)
# Create a binary classification: 'Acidic' for quality >= 7, 'Basic' otherwise
wine_data$quality <- ifelse(wine_data$quality >= 7, "Acidic", "Basic")
# Normalize the dataset (excluding the 'quality' column)
normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
wine_norm <- as.data.frame(lapply(wine_data[, -12], normalize))
# Add the quality column back to the normalized dataset
wine_norm$quality <- wine_data$quality
# Split the data into training and testing sets
set.seed(123)  # Ensure reproducibility
train_index <- createDataPartition(wine_norm$quality, p = 0.7, list = FALSE)
wine_train <- wine_norm[train_index, ]
wine_test <- wine_norm[-train_index, ]
# Implement KNN
knn_model <- knn(train = wine_train[, -12], test = wine_test[, -12], cl = wine_train$quality, k = 5)
# Evaluate KNN model
knn_conf_matrix <- confusionMatrix(knn_model, wine_test$quality)
print(knn_conf_matrix)
library(class)
library(e1071)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
data <- read.csv(url, sep = ";")
str(data)
library(class)
library(e1071)
library(class)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
data <- read.csv(url, sep = ";")
str(data)
head(data)
sum(is.na(data))
data$quality <- ifelse(data$quality >= 7, "acidic", "Basic")
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_norm <- as.data.frame(lapply(data[1:11], normalize))
data_norm$quality <- data$quality
set.seed(123)
train_indices <- sample(1:nrow(data_norm), 0.7 * nrow(data_norm))
train_data <- data_norm[train_indices, ]
test_data <- data_norm[-train_indices, ]
train_x <- train_data[, 1:11]
train_y <- train_data$quality
test_x <- test_data[, 1:11]
test_y <- test_data$quality
k <- 16
predicted_knn <- knn(train = train_x, test = test_x, cl = train_y, k = k)
accuracy_knn <- sum(predicted_knn == test_y) / length(test_y)
print(paste("KNN Accuracy:", round(accuracy_knn * 100, 2), "%"))
model_nb <- naiveBayes(train_x, train_y)
predicted_nb <- predict(model_nb, test_x)
accuracy_nb <- sum(predicted_nb == test_y) / length(test_y)
print(paste("Naive Bayes Accuracy:", round(accuracy_nb * 100, 2), "%"))
library(rpart.plot)
library(rpart.plot)
library(rpart)
data <- read.csv(file.choose())
data$diagnosis <- as.factor(data$diagnosis)
data$diagnosis <- as.factor(data$diagnosis)
data <- data[,-1]
set.seed(123)
train_index <- sample(seq_len(nrow(data)), size = 0.7 * nrow(data))
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
model <- rpart(diagnosis ~ ., data = train_data, method = "class")
rpart.plot(model)
predictions <- predict(model, test_data, type = "class")
conf_matrix <- table(test_data$diagnosis, predictions)
accuracy <- sum(predictions == test_data$diagnosis) / nrow(test_data)
print(paste("Accuracy:", accuracy))
print(paste("Accuracy:", accuracy*100)
print(paste("Accuracy:", accuracy*100)
accuracy <- sum(predictions == test_data$diagnosis) / nrow(test_data) * 100
print(paste("Accuracy:", accuracy)
print(paste("Accuracy:", accuracy)
accuracy <- sum(predictions == test_data$diagnosis) / nrow(test_data)
print(paste("Accuracy:", accuracy)
accuracy <- sum(predictions == test_data$diagnosis) / nrow(test_data) * 100
print(paste("Accuracy:", accuracy))
accuracy <- sum(predictions == test_data$diagnosis) / nrow(test_data)
print(paste("Accuracy:", accuracy))
#1. store the data into a data frame
data <- data.frame(
Years_Exp = c(1.1, 1.3, 1.5, 2.0, 2.2, 2.9, 3.0, 3.2, 3.2, 3.7),
Salary = c(39343.00, 46205.00, 37731.00, 43525.00,
39891.00, 56642.00, 60150.00, 54445.00, 64445.00, 57189.00)
)
?plot
#1. Storing the data into a data frame
data <- data.frame(
Hieght = c(5.1, 5.5, 5.8, 6.1, 6.4, 6.7, 6.4, 6.1, 5.10, 5.7),
Salary = c(63, 66, 69, 72, 75, 78, 75, 72, 69, 66)
)
#1. Storing the data into a data frame
data <- data.frame(
Hieght = c(5.1, 5.5, 5.8, 6.1, 6.4, 6.7, 6.4, 6.1, 5.10, 5.7),
Weight = c(63, 66, 69, 72, 75, 78, 75, 72, 69, 66)
)
#2. Creating the scatter plot
plot(data$Years_Exp, data$Salary,
xlab = "Height",
ylab = "Weight",
main = "Scatter Plot of Years Experienced vs Salary")
#2. Creating the scatter plot
plot(data$Height, data$Weight,
xlab = "Height",
ylab = "Weight",
main = "Scatter Plot of Years Experienced vs Salary")
#1. Storing the data into a data frame
data <- data.frame(
Hieght = c(5.1, 5.5, 5.8, 6.1, 6.4, 6.7, 6.4, 6.1, 5.10, 5.7),
Weight = c(63, 66, 69, 72, 75, 78, 75, 72, 69, 66)
)
#2. Creating the scatter plot
plot(data$Height, data$Weight,
xlab = "Height",
ylab = "Weight",
main = "Scatter Plot of Years Experienced vs Salary")
#2. Creating the scatter plot
plot(data$Height, data$Wright,
xlab = "Height",
ylab = "Weight",
main = "Height vs Weight"))
#2. Creating the scatter plot
plot(data$Height, data$Wright,
xlab = "Height",
ylab = "Weight",
main = "Height vs Weight")
#2. Creating the scatter plot
plot(data$Height, data$Wright,
xlab = "Height",
ylab = "Weight",
main = "Height vs Weight"))
#2. Creating the scatter plot
plot(data$Height, data$Weight,
xlab = "Height",
ylab = "Weight",
main = "Height vs Weight"))
#2. Creating the scatter plot
plot(data$Height, data$Weight)
#1. Storing the data into a data frame
data <- data.frame(
Years_Exp = c(1.1, 1.3, 1.5, 2.0, 2.2, 2.9, 3.0, 3.2, 3.2, 3.7),
Salary = c(39343.00, 46205.00, 37731.00, 43525.00,
39891.00, 56642.00, 60150.00, 54445.00, 64445.00, 57189.00)
)
plot(data$Years_Exp, data$Salary,
xlab = "Years Experienced",
ylab = "Salary",
main = "Scatter Plot of Years Experienced vs Salary")
relation <- lm(y~x)
relation <- lm(data$Years_Exp~data$Salary)
abline(relation, col = 'red', lwd = 2)
summary(relation)
a <- data.frame(x~2.3)
a <- data.frame(x=2.3)
sal <- predict(relation, a)
sal
x<- c(8,10,12)
y<- c(10,13,16)
plot(x,y)
relation<-lm(y~x)
abline(relation,col="red",lwd=2)
summary(relation)
summary(relation)
a<-data.frame(x=20)
weight<-predict(relation,a)
print(weight)
mouse.data <- data.frame(
size = c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3),
weight = c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
tail = c(0.7, 1.3, 0.7, 2.0, 3.6, 2.9, 3.9, 4.0)
)
mouse_data <- data.frame(
size = c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3),
weight = c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
tail = c(0.7, 1.3, 0.7, 2.0, 3.6, 2.9, 3.9, 4.0)
)
mouse_data <- data.frame(
size = c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3),
weight = c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
tail = c(0.7, 1.3, 0.7, 2.0, 3.6,3.0, 2.9, 3.9, 4.0)
)
plot(mouse_data)
data <- read.csv(file.choose())
Veiw(data)
View(data)
library(readr)
# Load the dataset
insurance_data <- read_csv("path/to/insurance.csv")
# Load the dataset
insurance_data <- read_csv(file.choose())
# Convert categorical variables to factors
insurance_data$sex <- as.factor(insurance_data$sex)
insurance_data$smoker <- as.factor(insurance_data$smoker)
insurance_data$region <- as.factor(insurance_data$region)
# Fit a multiple linear regression model
model <- lm(charges ~ age + sex + bmi + children + smoker + region, data = insurance_data)
# Summarize the model
summary(model)
example_data <- data.frame(
Experience = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
Education = c(12, 14, 16, 14, 12, 16, 14, 12, 14, 16),
Age = c(21, 25, 24, 26, 23, 27, 29, 28, 30, 32),
Salary = c(50, 55, 60, 62, 58, 65, 68, 70, 72, 75)
)
example_data <- as.factor(example_data$Experience)
# Convert categorical variables to factors
insurance_data$sex <- as.factor(insurance_data$sex)
insurance_data$smoker <- as.factor(insurance_data$smoker)
insurance_data$region <- as.factor(insurance_data$region)
# Descriptive statistics using psych package
describe(insurance_data)
# Install and load the necessary packages
#install.packages("psych")
library(psych)
library(readr)
# Descriptive statistics using psych package
describe(insurance_data)
# Fit a multiple linear regression model
model <- lm(charges ~ age + sex + bmi + children + smoker + region, data = insurance_data)
# Summarize the model
summary(model)
# Install and load necessary packages
#install.packages("psych")
library(psych)
# Create the example dataset
example_data <- data.frame(
Experience = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),
Education = c(12, 14, 16, 14, 12, 16, 14, 12, 14, 16),
Age = c(21, 25, 24, 26, 23, 27, 29, 28, 30, 32),
Salary = c(50, 55, 60, 62, 58, 65, 68, 70, 72, 75)
)
# Descriptive statistics using psych package
describe(example_data)
# Fit a multiple linear regression model
model <- lm(Salary ~ Experience + Education + Age, data = example_data)
# Summarize the model
summary(model)
# Load necessary libraries
library(dplyr)
# Load the dataset
data <- read.csv("insurance.csv")
# Load the dataset
data <- read.csv(file.choose())
# Convert categorical variables into factors
data$sex <- as.factor(data$sex)
data$smoker <- as.factor(data$smoker)
data$region <- as.factor(data$region)
# Build the linear regression model
model <- lm(charges ~ age + bmi + children + sex + smoker + region, data = data)
# Summary of the model
summary(model)
# Predict on the same dataset (or you can split data for training/testing)
predictions <- predict(model, newdata = data)
# Calculate Mean Squared Error (MSE)
mse <- mean((data$charges - predictions)^2)
# Display MSE
print(paste("Mean Squared Error:", mse))
Experience = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
Education = c(12, 14, 16, 14, 12, 16, 14, 12, 14, 16)
Age = c(21, 25, 24, 26, 23, 27, 29, 28, 30, 32)
Salary = c(50, 55, 60, 62, 58, 65, 68, 70, 72, 75)
data<-data.frame(Experience,Education,Age,Salary)
plot(data)
relation<-lm(Salary~Experience+Education+Age)
summary(relation)
pairs.panels(data[c("Experience","Education","Age","Salary")])
new<-data.frame(Experience=15,Education=16,Age=34)
sal_pred<-predict(relation,new)
sal_pred
?pairs.panels
?ggplot2
####
# data
example_data <- data.frame(
Age = c(21, 25, 24, 26, 23, 27, 29, 28, 30, 32),
Salary = c(50, 55, 60, 62, 58, 65, 68, 70, 72, 75)
)
####
# data
example_data <- read.csv(file.choose())
#fitting polynomial regression to the dataset
example_data$Level2 = example_data$Level^2
example_data$Level3 = example_data$Level^3
example_data$Level4 = example_data$Level^4
poly_reg <- lm(formula = Salary ~., data = example_data)
summary(poly_reg)
example_data$Level
example_data$Level2
example_data$Level3
example_data$Level4
#Visual part
require(ggplot2)
lin_reg <- lm(formula = Salary, data = example_data)
lin_reg <- lm(formula = Salary ~., data = example_data)
ggplot() + geom_point(aes(x = example_data$Level, y = example_data$Salary),
colour = 'black') +
geom_line(aes(x = example_data$Level, y = predict(lin_reg, newdata = example_data)),
colour = 'purple') +
ggtitle('(linear regression') +
xlab('level') +
ylab('salary')
lin_reg <- lm(formula = Salary, data = example_data)
lin_reg <- lm(formula = Salary ~., data = example_data)
ggplot() + geom_point(aes(x = example_data$Level, y = example_data$Salary),
colour = 'black') +
geom_line(aes(x = example_data$Level, y = predict(lin_reg, newdata = example_data)),
colour = 'purple') +
ggtitle('(linear regression') +
xlab('level') +
ylab('salary')
ggplot() + geom_point(aes(x = example_data$Level, y = example_data$Salary),
colour = 'black') +
geom_line(aes(x = example_data$Level, y = predict(lin_reg, newdata = example_data)),
colour = 'purple') +
ggtitle('linear regression') +
xlab('level') +
ylab('salary')
ggplot() + geom_point(aes(x = example_data$Level, y = example_data$Salary),
colour = 'black') +
geom_line(aes(x = example_data$Level, y = predict(lin_reg, newdata = example_data)),
colour = 'purple') +
ggtitle('poly regression') +
xlab('level') +
ylab('salary')
ggplot() + geom_point(aes(x = example_data$Level, y = example_data$Salary),
colour = 'black') +
geom_line(aes(x1 = example_data$Level, y1 = predict(lin_reg, newdata = example_data)),
colour = 'purple') +
ggtitle('poly regression') +
xlab('level') +
ylab('salary')
ggplot() + geom_point(aes(x = example_data$Level, y = example_data$Salary),
colour = 'black') +
geom_line(aes(x = example_data$Level, y = predict(lin_reg, newdata = example_data)),
colour = 'purple') +
ggtitle('poly regression') +
xlab('level') +
ylab('salary')
predictions = predict(lin_reg, newdata = example_data)
mse <- mean((example_data$Salary - predictions)^2)
mse
# Calculate Mean Squared Error (MSE)
mse <- mean((data$charges - predictions)^2)
# Display MSE
print(paste("Mean Squared Error:", mse))
data -> file.choose(read.csv())
data <- file.choose(read.csv())
data <- file.choose(read.csv())
data <- read.csv(file.choose())
svm_data -> read.csv(file.choose())
svm_data <- read.csv(file.choose())
head(svm_data)
View(svm_data)
str(svm_data)
svm_data = svm_data[3:5]
#encode the target feature into factor type
svm_data$Purchased = factor(svm_data$Purchased, levels = c(0,1))
View(svm_data)
#spitting the data into training and testing
install.packages('caTools')
require(caTools)
?caTools
?`caTools-package`
split_svm <- sample.split(svm_data, SplitRatio = 0.75)
training_data <- subset(svm_data, split = TRUE)
training_data <- subset(svm_data, split(TRUE))
training_data <- subset(svm_data, split == TRUE)
training_data <- subset(svm_data, split_svm == TRUE)
test_data <- subset(svm_data, split_svm == FALSE)
View(training_data)
View(test_data)
View(svm_data)
View(split_svm)
#feature scaling on training and testing data, like how we do normalization
training_data[-3] <- scale(training_data[-3])
test_data[-3] <- scale(test_data[-3])
# Fitting SVM to the Training set
require(e1071)
svm_classifier = svm(formula = Purchased ~ .,
data = training_data,
type = 'C-classification',
kernel = 'linear')
View(svm_classifier)
svm_classifier
# Predicting the Test set results
y_pred = predict(svm_classifier, newdata = test_set[-3])
# Predicting the Test set results
y_pred = predict(svm_classifier, newdata = test_data[-3])
# Making the Confusion Matrix
cm = table(test_data[, 3], y_pred)
# Making the Confusion Matrix
cm = table(test_data[, 3], y_pred)
cm
version
knn_labels <- factor(new_knn$diagnosis, levels = c("M", "B"),
labels = c("Malignant", "Benign"))
new_knn <- knn_data[-1]
setwd("D:\\asus_cloud\\Desktop\\B.tech\\FIFTH SEM\\predictive analysis\\DATA")
knn_data <- read.csv(file.choose(), stringsAsFactors = FALSE)
new_knn <- knn_data[-1]
View(new_knn)
another_knn <- new_knn
another_knn <- new_knn[-2]
View(another_knn)
another_knn <- new_knn[-1]
View(another_knn)
View(new_knn)
knn_labels <- factor(new_knn$diagnosis, levels = c("M", "B"),
labels = c("Malignant", "Benign"))
knn_labels
summary(new_knn[c("area_mean", "smoothness_mean", "compactness_mean")])
# Mam's code:
getwd()
# Mam's code:
getwd()
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
str (wbcd)
wbcd <- wbcd[-1]
table(wbcdsdiagnosis)
wbcdSdiagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant"))
table(wbcdsdiagnosis)
round(prop.table(table(wbcdSdiagnosis)) * 100, digits = 1)
?table
round(prop.table(table(wbcdSdiagnosis)) * 100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_nSarea_mean)
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
wbcd_test_labels <- wbcd[470:569, 1]
wbcd_train_labels <- wbcd[1:469, 1]
library(class)
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)
library(gmodels)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq=FALSE)
aa <- table(wbcd_test_labels,wbcd_test_pred)
library(caret)
confusionMatrix(aa)
